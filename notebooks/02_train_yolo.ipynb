{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425ded43",
   "metadata": {},
   "source": [
    "YOLO Model Configuration and Setup\n",
    "\n",
    "This notebook defines the YOLO model architecture and training hyperparameters for Pascal VOC 2012 dataset. It is configuration-only - prepares all parameters for training in notebook 03. No training execution here.\n",
    "\n",
    "Configuration Coverage:\n",
    "1. YOLO model selection (v8n - nano variant)\n",
    "2. Dataset and class definition (3 classes: person, car, dog from Pascal VOC)\n",
    "3. data.yaml configuration with normalized bounding boxes\n",
    "4. Model architecture parameters (input size 416x416, batch size 16)\n",
    "5. Training hyperparameters optimized for large dataset (50 epochs)\n",
    "6. Reproducibility settings (seed 42)\n",
    "\n",
    "Dataset: Pascal VOC 2012 (~3000-5000 images after filtering)\n",
    "Training Time: 30-60 minutes on GPU, 3-4 hours on CPU\n",
    "Output: Model ready for inference in notebook 04\n",
    "\n",
    "This notebook is prerequisite for notebook 03_training.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4170be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO TRAINING PIPELINE\n",
      "==================================================\n",
      "Device: cpu\n",
      "Classes: 3 (person, car, dog)\n",
      "Project Root: C:\\Users\\mlata\\Documents\\iajordy2\n",
      "Data Dir: C:\\Users\\mlata\\Documents\\iajordy2\\data\n",
      "Models Dir: C:\\Users\\mlata\\Documents\\iajordy2\\models\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Project structure\n",
    "PROJECT_ROOT = Path('../')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"YOLO MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data Dir: {DATA_DIR}\")\n",
    "print(f\"Models Dir: {MODELS_DIR}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3c548",
   "metadata": {},
   "source": [
    "Stage 1: Environment Setup and Paths\n",
    "\n",
    "This stage initializes the Python environment by setting up file paths and directories required for the training configuration.\n",
    "\n",
    "Configuration components:\n",
    "- SEED: Fixed random seed (42) for reproducibility across runs\n",
    "- PROJECT_ROOT: Base directory containing all project files\n",
    "- DATA_DIR: Location of prepared YOLO dataset from notebook 01\n",
    "- MODELS_DIR: Directory where trained models will be saved\n",
    "\n",
    "All paths use pathlib.Path for cross-platform compatibility (Windows, Linux, macOS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbff3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.9  Python-3.10.0 torch-2.10.0+cpu CPU (AMD Ryzen 9 5900XT 16-Core Processor)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:\\Users\\mlata\\Documents\\iajordy2\\data\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolo_run, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:\\Users\\mlata\\Documents\\iajordy2\\models, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, 16, None, [64, 128, 256]] \n",
      "Model summary: 130 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 952.782.3 MB/s, size: 102.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\mlata\\Documents\\iajordy2\\data\\labels\\train.cache... 400 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 400/400  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1047.5117.3 MB/s, size: 102.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\mlata\\Documents\\iajordy2\\data\\labels\\val.cache... 50 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 50/50  0.0s\n",
      "Plotting labels to C:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/01 12:55:42 INFO mlflow.tracking.fluent: Experiment with name 'C:\\Users\\mlata\\Documents\\iajordy2\\models' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(a4667a31980e41ea9f39506ccc9d600f) to file:///mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "WARNING \u001b[34m\u001b[1mMLflow: \u001b[0mFailed to initialize: Changing param values is not allowed. Param with key='model' was already logged with value='yolov8n' for run ID='a4667a31980e41ea9f39506ccc9d600f'. Attempted logging new value 'yolov8n.pt'.\n",
      "WARNING \u001b[34m\u001b[1mMLflow: \u001b[0mNot tracking this run\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50         0G      2.999      3.976      2.654         54        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 23.8s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.5it/s 1.4s2.9s\n",
      "                   all         50         99    0.00133      0.289    0.00155   0.000365\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50         0G      2.863      3.883      2.518         50        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 23.2s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.8it/s 1.1s2.4s\n",
      "                   all         50         99      0.002      0.305    0.00196   0.000438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50         0G       2.82      3.833      2.504         69        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.6s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7it/s 1.2s2.5s\n",
      "                   all         50         99    0.00199      0.306    0.00321   0.000841\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50         0G      2.775      3.811      2.477         52        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.9s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.6it/s 1.3s2.8s\n",
      "                   all         50         99    0.00219      0.333     0.0146    0.00426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50         0G      2.762      3.786       2.46         55        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.5s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.0s/it 2.1s4.4s\n",
      "                   all         50         99    0.00189      0.269    0.00168   0.000376\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50         0G      2.691      3.851      2.445         65        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 23.4s1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.8s/it 3.5s7.6s\n",
      "                   all         50         99   0.000832      0.139   0.000504   7.18e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50         0G      2.671      3.846      2.434         65        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.3s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.6it/s 1.3s2.7s\n",
      "                   all         50         99    0.00292      0.451    0.00257    0.00058\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50         0G      2.668      3.864      2.399         53        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.5s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.6it/s 1.3s2.7s\n",
      "                   all         50         99    0.00224      0.351    0.00229   0.000503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50         0G      2.581      3.835      2.361         62        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.7s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7it/s 1.2s2.6s\n",
      "                   all         50         99     0.0023      0.353    0.00213   0.000484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50         0G      2.597       3.81       2.41         51        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.5s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7s/it 3.5s7.5s\n",
      "                   all         50         99    0.00109      0.172   0.000719   0.000177\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50         0G      2.662      3.824        2.4         61        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.7s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 2.0s/it 3.9s8.4s\n",
      "                   all         50         99    0.00156      0.204    0.00119   0.000254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/50         0G      2.593      3.797       2.37         63        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.4s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7s/it 3.4s7.4s\n",
      "                   all         50         99    0.00101       0.19   0.000963   0.000192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/50         0G      2.571      3.805      2.363         55        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 22.5s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7it/s 1.2s2.6s\n",
      "                   all         50         99    0.00313      0.467    0.00529    0.00135\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/50         0G      2.569       3.79      2.401         56        416: 100% ━━━━━━━━━━━━ 25/25 1.1it/s 23.8s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.5s3.2s\n",
      "                   all         50         99    0.00339      0.505    0.00667    0.00142\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 4, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "14 epochs completed in 0.097 hours.\n",
      "Optimizer stripped from C:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from C:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating C:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\\weights\\best.pt...\n",
      "Ultralytics 8.4.9  Python-3.10.0 torch-2.10.0+cpu CPU (AMD Ryzen 9 5900XT 16-Core Processor)\n",
      "Model summary (fused): 73 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.5it/s 1.4s2.9s\n",
      "                   all         50         99    0.00226      0.344     0.0146    0.00425\n",
      "                person         28         36    0.00279      0.389     0.0025   0.000655\n",
      "                   car         26         33    0.00169      0.242    0.00151   0.000346\n",
      "                   dog         26         30    0.00229        0.4     0.0397     0.0118\n",
      "Speed: 0.5ms preprocess, 20.0ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Best model saved: C:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\\weights\\best.pt\n",
      "Training completed with MLflow tracking\n",
      "Model location: C:\\Users\\mlata\\Documents\\iajordy2\\models\\yolo_run\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "# Model selection\n",
    "MODEL_NAME = 'yolov8n'\n",
    "PRETRAINED_WEIGHTS = 'yolov8n.pt'\n",
    "\n",
    "print(\"\\n[1] Model Selection\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Pretrained: COCO\")\n",
    "print(f\"Weights: {PRETRAINED_WEIGHTS}\")\n",
    "print(f\"Architecture: Nano (efficient, fast)\")\n",
    "print(f\"Use case: Object detection with 3 classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66920ece",
   "metadata": {},
   "source": [
    "Stage 2: Model Selection\n",
    "\n",
    "This stage specifies the YOLO model architecture and pretrained weights to use.\n",
    "\n",
    "YOLOv8 variants by size:\n",
    "- yolov8n (nano): 3.2M parameters - fast and efficient\n",
    "- yolov8s (small): 11.2M parameters - balanced speed/accuracy\n",
    "- yolov8m (medium): 25.9M parameters - good accuracy\n",
    "- yolov8l (large): 43.7M parameters - high accuracy\n",
    "- yolov8x (xlarge): 68.2M parameters - best accuracy but slower\n",
    "\n",
    "This notebook uses yolov8n because:\n",
    "1. Lightweight for remote environments with limited resources\n",
    "2. Pretrained on COCO (80 classes) provides good transfer learning base\n",
    "3. Will be fine-tuned to our 3-class detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d694292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.9  Python-3.10.0 torch-2.10.0+cpu CPU (AMD Ryzen 9 5900XT 16-Core Processor)\n",
      "Model summary (fused): 73 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 899.362.2 MB/s, size: 102.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\mlata\\Documents\\iajordy2\\data\\labels\\val.cache... 50 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 50/50  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 3.2it/s 1.3s0.6s\n",
      "                   all         50         99    0.00226      0.344     0.0146    0.00425\n",
      "Speed: 0.4ms preprocess, 18.0ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\val\u001b[0m\n",
      "Validation Metrics:\n",
      "  mAP50: 0.0146\n",
      "  mAP50_95: 0.0043\n",
      "  precision: 0.0023\n",
      "  recall: 0.3438\n",
      "Validation completed\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "NUM_CLASSES = 3\n",
    "CLASS_NAMES = ['person', 'car', 'dog']\n",
    "CLASS_MAPPING = {i: name for i, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "print(\"\\n[2] Dataset Configuration\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {', '.join(CLASS_NAMES)}\")\n",
    "print(f\"Format: YOLO (normalized bounding boxes)\")\n",
    "print(f\"Splits: train, val, test\")\n",
    "print(\"\\nClass IDs:\")\n",
    "for class_id, class_name in CLASS_MAPPING.items():\n",
    "    print(f\"  {class_id}: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160d36b",
   "metadata": {},
   "source": [
    "Stage 3: Dataset Configuration\n",
    "\n",
    "This stage defines the dataset composition and class mappings for the training task.\n",
    "\n",
    "Dataset specification:\n",
    "- NUM_CLASSES: 3 target object classes\n",
    "- CLASS_NAMES: Names of classes (person, car, dog)\n",
    "- Format: YOLO normalized bounding boxes (as prepared in notebook 01)\n",
    "\n",
    "Class ID mapping:\n",
    "- Class ID is the index position in CLASS_NAMES list\n",
    "- Stored in first column of label files\n",
    "- Used by YOLO model to identify object categories during training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6252a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/01 13:01:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/01 13:01:49 WARNING mlflow.pytorch: Saving pytorch model by Pickle or CloudPickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is to set 'export_model' to True to save the pytorch model using the safe graph model format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registered at: file:///c:/mlruns/980429098773043767/5a6580d8183242a39dfda7d1ce3c132f/artifacts/yolo_model\n",
      "Model transitioned to Production - Version: 1\n",
      "Model Registration completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlata\\Documents\\iajordy2\\.venv\\lib\\site-packages\\mlflow\\tracking\\_model_registry\\utils.py:216: FutureWarning: The filesystem model registry backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri)\n",
      "Successfully registered model 'yolo_3class_detector'.\n",
      "Created version '1' of model 'yolo_3class_detector'.\n",
      "C:\\Users\\mlata\\AppData\\Local\\Temp\\ipykernel_4484\\1196882377.py:16: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  model_version = client.get_latest_versions('yolo_3class_detector')[0]\n",
      "C:\\Users\\mlata\\AppData\\Local\\Temp\\ipykernel_4484\\1196882377.py:18: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    }
   ],
   "source": [
    "# Create/Update data.yaml for YOLO\n",
    "data_yaml_path = DATA_DIR / 'data.yaml'\n",
    "\n",
    "data_yaml_content = {\n",
    "    'path': str(DATA_DIR.absolute()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'test': 'images/test',\n",
    "    'nc': NUM_CLASSES,\n",
    "    'names': CLASS_NAMES\n",
    "}\n",
    "\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"\\n[3] Data Configuration (data.yaml)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Path: {data_yaml_path}\")\n",
    "print(f\"\\nContent:\")\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a3f66",
   "metadata": {},
   "source": [
    "Stage 4: Data Configuration File Generation\n",
    "\n",
    "This stage creates the data.yaml file, which is the interface between the dataset and YOLO training.\n",
    "\n",
    "data.yaml is required by YOLO and specifies:\n",
    "- path: Absolute path to dataset base directory\n",
    "- train: Relative path to training images\n",
    "- val: Relative path to validation images\n",
    "- test: Relative path to test images\n",
    "- nc: Number of classes\n",
    "- names: List of class names\n",
    "\n",
    "This file is created during notebook 01 but verified/recreated here for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters (to be used in notebook 03)\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 16,\n",
    "    'imgsz': 416,\n",
    "    'patience': 10,\n",
    "    'device': 'cuda',  # or 'cpu' if GPU not available\n",
    "    'seed': SEED,\n",
    "    'lr0': 0.01,  # initial learning rate\n",
    "    'lrf': 0.01,  # final learning rate\n",
    "    'momentum': 0.937,\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 3.0,\n",
    "    'warmup_momentum': 0.8,\n",
    "    'verbose': True,\n",
    "    'save': True,\n",
    "    'exist_ok': True\n",
    "}\n",
    "\n",
    "print(\"\\n[4] Training Hyperparameters\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Configuration (to be used in notebook 03):\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Configuration complete\")\n",
    "print(\"Ready for training in notebook 03_training.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d4f85",
   "metadata": {},
   "source": [
    "Stage 5: Training Hyperparameters Definition\n",
    "\n",
    "This stage defines all hyperparameters that control the training process executed in notebook 03. Optimized for lightweight academic dataset.\n",
    "\n",
    "Key hyperparameters:\n",
    "- epochs: 20 complete passes through training data (reduced from 50 for small dataset)\n",
    "- batch_size: 8 images per gradient update (reduced from 16 for small dataset)\n",
    "- imgsz: Input image size 416x416 pixels\n",
    "- patience: Early stopping at 5 epochs without improvement (reduced from 10)\n",
    "- device: GPU (cuda) or CPU for training\n",
    "- seed: Fixed random seed 42 for reproducibility\n",
    "- learning rate (lr0, lrf): Initial 0.01, final 0.001 with decay schedule\n",
    "- momentum: SGD momentum 0.937 for optimization\n",
    "- weight_decay: L2 regularization 0.0005 to prevent overfitting\n",
    "- warmup: Gradual learning rate increase in first 2 epochs\n",
    "\n",
    "For academic lightweight dataset:\n",
    "- Smaller number of epochs prevents overfitting\n",
    "- Reduced batch size matches small dataset size\n",
    "- Shorter early stopping patience\n",
    "- Total training time: 5-15 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
