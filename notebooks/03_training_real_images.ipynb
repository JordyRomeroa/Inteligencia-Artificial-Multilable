{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de0cc12",
   "metadata": {},
   "source": [
    "# Entrenamiento Multilabel con PASCAL VOC 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f96431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU disponible: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd8430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion:\n",
      "  Tamaño imagen: (224, 224)\n",
      "  Batch size: 16\n",
      "  Epocas inicial: 30\n",
      "  Epocas fine-tuning: 40\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'voc2007'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "INITIAL_EPOCHS = 30\n",
    "FINETUNING_EPOCHS = 40\n",
    "LEARNING_RATE_INITIAL = 0.0005\n",
    "LEARNING_RATE_FINETUNING = 0.00005\n",
    "\n",
    "print(f\"Configuracion:\")\n",
    "print(f\"  Tamaño imagen: {IMG_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epocas inicial: {INITIAL_EPOCHS}\")\n",
    "print(f\"  Epocas fine-tuning: {FINETUNING_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8060b57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando desde: c:\\Users\\mlata\\Documents\\iajordy2\\data\\voc2007\n",
      "Clases cargadas: 20\n",
      "Primeras 10 clases: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cargando desde: {DATA_DIR}\")\n",
    "\n",
    "with open(DATA_DIR / 'classes.json', 'r') as f:\n",
    "    classes = json.load(f)\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(f\"Clases cargadas: {NUM_CLASSES}\")\n",
    "print(f\"Primeras 10 clases: {classes[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac471230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset PASCAL VOC 2007 desde NPZ...\n",
      "Imagenes cargadas: (2501, 224, 224, 3)\n",
      "Labels cargados: (2501, 20)\n",
      "Clases por imagen (promedio): 1.61\n",
      "Imagenes normalizadas a rango [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando dataset PASCAL VOC 2007 desde NPZ...\")\n",
    "\n",
    "# Cargar NPZ\n",
    "npz_file = DATA_DIR / 'voc2007_multilabel.npz'\n",
    "if not npz_file.exists():\n",
    "    raise FileNotFoundError(f\"No se encuentra {npz_file}. Ejecuta primero 01_data_analysis.ipynb\")\n",
    "\n",
    "data = np.load(npz_file)\n",
    "images = data['images']\n",
    "labels = data['labels']\n",
    "\n",
    "print(f\"Imagenes cargadas: {images.shape}\")\n",
    "print(f\"Labels cargados: {labels.shape}\")\n",
    "print(f\"Clases por imagen (promedio): {labels.sum(axis=1).mean():.2f}\")\n",
    "\n",
    "# Normalizar imagenes a [0, 1]\n",
    "images = images.astype(np.float32) / 255.0\n",
    "\n",
    "print(f\"Imagenes normalizadas a rango [0, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "167c4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1750 imágenes\n",
      "Val: 375 imágenes\n",
      "Test: 376 imágenes\n",
      "Train labels: 1.62 categorías/imagen\n",
      "Test labels: 1.59 categorías/imagen\n",
      "Val labels: 1.57 categorías/imagen\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    images, labels, test_size=0.3, random_state=SEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} imágenes\")\n",
    "print(f\"Val: {len(X_val)} imágenes\")\n",
    "print(f\"Test: {len(X_test)} imágenes\")\n",
    "\n",
    "print(f\"Train labels: {y_train.sum(axis=1).mean():.2f} categorías/imagen\")\n",
    "print(f\"Test labels: {y_test.sum(axis=1).mean():.2f} categorías/imagen\")\n",
    "print(f\"Val labels: {y_val.sum(axis=1).mean():.2f} categorías/imagen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddf787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos por clase calculados (limitados a max 10)\n",
      "  Min: 1.30\n",
      "  Max: 10.00\n",
      "  Media: 9.20\n",
      "Focal Loss con class weights definida (gamma=2.0)\n"
     ]
    }
   ],
   "source": [
    "# Calcular pesos por clase para combatir desbalance\n",
    "pos_counts = y_train.sum(axis=0)\n",
    "neg_counts = y_train.shape[0] - pos_counts\n",
    "\n",
    "# Peso positivo = negativos / positivos (LIMITADO a max 3)\n",
    "pos_weight = (neg_counts + 1e-6) / (pos_counts + 1e-6)\n",
    "pos_weight = np.clip(pos_weight, 1.0, 3.0)  # Max 3 para evitar over-compensación\n",
    "\n",
    "class_weights = tf.constant(pos_weight, dtype=tf.float32)\n",
    "\n",
    "print(\"Pesos por clase calculados (limitados a max 3)\")\n",
    "print(f\"  Min: {pos_weight.min():.2f}\")\n",
    "print(f\"  Max: {pos_weight.max():.2f}\")\n",
    "print(f\"  Media: {pos_weight.mean():.2f}\")\n",
    "\n",
    "# Binary Cross-Entropy Weighted (más estable que Focal Loss)\n",
    "def weighted_bce_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy con class weights.\n",
    "    Más estable que Focal Loss para evitar predecir todo como positivo.\n",
    "    \"\"\"\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # BCE componentes\n",
    "    bce = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "    \n",
    "    # Aplicar class weights solo a positivos\n",
    "    weighted_bce = bce * (y_true * class_weights + (1 - y_true) * 1.0)\n",
    "    \n",
    "    return tf.reduce_mean(weighted_bce)\n",
    "\n",
    "print(\"Weighted BCE Loss definida (más estable que Focal Loss)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f838d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generadores de datos creados\n",
      "  Train samples: 1750\n",
      "  Val samples: 375\n",
      "  Test samples: 376\n",
      "  Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation para training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Sin augmentation para val/test (ya están normalizadas)\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Fit datagen en datos de train\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "print(\"Generadores de datos creados\")\n",
    "print(f\"  Train samples: {len(X_train)}\")\n",
    "print(f\"  Val samples: {len(X_val)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97110162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo creado\n",
      "Total parametros: 4,841,911\n"
     ]
    }
   ],
   "source": [
    "def create_multilabel_model(num_classes, img_size=(224, 224)):\n",
    "    inputs = layers.Input(shape=(*img_size, 3))\n",
    "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    base_model.trainable = False\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model, base_model\n",
    "\n",
    "model, base_model = create_multilabel_model(NUM_CLASSES, IMG_SIZE)\n",
    "print(f\"Modelo creado\")\n",
    "print(f\"Total parametros: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f57405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo compilado - FASE 1: Training inicial con Focal Loss\n",
      "Epoch 1/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.4734 - loss: 0.2623 - precision: 0.1091 - recall: 0.3950\n",
      "Epoch 1: val_loss improved from None to 0.24407, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 215ms/step - auc: 0.4928 - loss: 0.2557 - precision: 0.1145 - recall: 0.3543 - val_auc: 0.5011 - val_loss: 0.2441 - val_precision: 0.2262 - val_recall: 0.4312 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - auc: 0.4901 - loss: 0.2488 - precision: 0.1323 - recall: 0.3107\n",
      "Epoch 2: val_loss improved from 0.24407 to 0.24401, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 205ms/step - auc: 0.4952 - loss: 0.2479 - precision: 0.1349 - recall: 0.2759 - val_auc: 0.5046 - val_loss: 0.2440 - val_precision: 0.2231 - val_recall: 0.4228 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - auc: 0.4866 - loss: 0.2465 - precision: 0.1505 - recall: 0.2287\n",
      "Epoch 3: val_loss improved from 0.24401 to 0.24299, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 221ms/step - auc: 0.4908 - loss: 0.2466 - precision: 0.1618 - recall: 0.2303 - val_auc: 0.5009 - val_loss: 0.2430 - val_precision: 0.3956 - val_recall: 0.2767 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - auc: 0.4855 - loss: 0.2472 - precision: 0.1910 - recall: 0.2551\n",
      "Epoch 4: val_loss improved from 0.24299 to 0.24157, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 231ms/step - auc: 0.5030 - loss: 0.2451 - precision: 0.1892 - recall: 0.2413 - val_auc: 0.5011 - val_loss: 0.2416 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - auc: 0.4906 - loss: 0.2441 - precision: 0.2229 - recall: 0.2676\n",
      "Epoch 5: val_loss did not improve from 0.24157\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 223ms/step - auc: 0.4975 - loss: 0.2456 - precision: 0.2129 - recall: 0.2360 - val_auc: 0.4974 - val_loss: 0.2434 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.4959 - loss: 0.2438 - precision: 0.1967 - recall: 0.1587\n",
      "Epoch 6: val_loss did not improve from 0.24157\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 199ms/step - auc: 0.4910 - loss: 0.2452 - precision: 0.1895 - recall: 0.1505 - val_auc: 0.5001 - val_loss: 0.2416 - val_precision: 0.1253 - val_recall: 0.1596 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - auc: 0.4911 - loss: 0.2438 - precision: 0.2107 - recall: 0.1763\n",
      "Epoch 7: val_loss improved from 0.24157 to 0.24143, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 199ms/step - auc: 0.4956 - loss: 0.2443 - precision: 0.2255 - recall: 0.1932 - val_auc: 0.5000 - val_loss: 0.2414 - val_precision: 0.0960 - val_recall: 0.0611 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - auc: 0.4921 - loss: 0.2421 - precision: 0.2314 - recall: 0.1678\n",
      "Epoch 8: val_loss did not improve from 0.24143\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 198ms/step - auc: 0.4988 - loss: 0.2437 - precision: 0.2303 - recall: 0.1568 - val_auc: 0.5000 - val_loss: 0.2416 - val_precision: 0.0960 - val_recall: 0.0611 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.4900 - loss: 0.2450 - precision: 0.2328 - recall: 0.2053\n",
      "Epoch 9: val_loss improved from 0.24143 to 0.24066, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 200ms/step - auc: 0.4941 - loss: 0.2435 - precision: 0.2365 - recall: 0.2268 - val_auc: 0.5000 - val_loss: 0.2407 - val_precision: 0.2587 - val_recall: 0.3294 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - auc: 0.4890 - loss: 0.2428 - precision: 0.2302 - recall: 0.2233\n",
      "Epoch 10: val_loss improved from 0.24066 to 0.24048, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 203ms/step - auc: 0.4864 - loss: 0.2435 - precision: 0.2363 - recall: 0.2229 - val_auc: 0.5000 - val_loss: 0.2405 - val_precision: 0.2240 - val_recall: 0.4278 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - auc: 0.4920 - loss: 0.2433 - precision: 0.2178 - recall: 0.2141\n",
      "Epoch 11: val_loss did not improve from 0.24048\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 206ms/step - auc: 0.4962 - loss: 0.2432 - precision: 0.2193 - recall: 0.2123 - val_auc: 0.5000 - val_loss: 0.2406 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.4884 - loss: 0.2441 - precision: 0.2448 - recall: 0.2474\n",
      "Epoch 12: val_loss improved from 0.24048 to 0.24035, saving model to c:\\Users\\mlata\\Documents\\iajordy2\\models\\model_phase1_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 201ms/step - auc: 0.4882 - loss: 0.2432 - precision: 0.2496 - recall: 0.2384 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.4213 - val_recall: 0.2683 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.4881 - loss: 0.2398 - precision: 0.2693 - recall: 0.2411\n",
      "Epoch 13: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 199ms/step - auc: 0.4937 - loss: 0.2430 - precision: 0.2650 - recall: 0.2310 - val_auc: 0.5000 - val_loss: 0.2405 - val_precision: 0.2587 - val_recall: 0.3294 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.4903 - loss: 0.2403 - precision: 0.2355 - recall: 0.2499\n",
      "Epoch 14: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 200ms/step - auc: 0.4867 - loss: 0.2431 - precision: 0.2217 - recall: 0.2296 - val_auc: 0.5000 - val_loss: 0.2405 - val_precision: 0.2587 - val_recall: 0.3294 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - auc: 0.4971 - loss: 0.2401 - precision: 0.2708 - recall: 0.2491\n",
      "Epoch 15: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 201ms/step - auc: 0.4969 - loss: 0.2430 - precision: 0.2582 - recall: 0.2381 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.2240 - val_recall: 0.4278 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.4820 - loss: 0.2445 - precision: 0.2231 - recall: 0.2161\n",
      "Epoch 16: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 199ms/step - auc: 0.4921 - loss: 0.2431 - precision: 0.2319 - recall: 0.2314 - val_auc: 0.5000 - val_loss: 0.2405 - val_precision: 0.2240 - val_recall: 0.4278 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - auc: 0.4888 - loss: 0.2418 - precision: 0.2406 - recall: 0.2160\n",
      "Epoch 17: val_loss did not improve from 0.24035\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 197ms/step - auc: 0.4885 - loss: 0.2431 - precision: 0.2430 - recall: 0.2134 - val_auc: 0.5000 - val_loss: 0.2405 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.4937 - loss: 0.2413 - precision: 0.2534 - recall: 0.2368\n",
      "Epoch 18: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 198ms/step - auc: 0.4943 - loss: 0.2430 - precision: 0.2540 - recall: 0.2356 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 2.5000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - auc: 0.4909 - loss: 0.2413 - precision: 0.2422 - recall: 0.2343\n",
      "Epoch 19: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 198ms/step - auc: 0.4974 - loss: 0.2429 - precision: 0.2506 - recall: 0.2391 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 2.5000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.4886 - loss: 0.2472 - precision: 0.2358 - recall: 0.2232\n",
      "Epoch 20: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 199ms/step - auc: 0.4950 - loss: 0.2429 - precision: 0.2259 - recall: 0.2264 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.1547 - val_recall: 0.0985 - learning_rate: 2.5000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - auc: 0.4959 - loss: 0.2408 - precision: 0.2424 - recall: 0.2127\n",
      "Epoch 21: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 197ms/step - auc: 0.4994 - loss: 0.2429 - precision: 0.2485 - recall: 0.2158 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.1547 - val_recall: 0.0985 - learning_rate: 2.5000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - auc: 0.5051 - loss: 0.2443 - precision: 0.2306 - recall: 0.2063\n",
      "Epoch 22: val_loss did not improve from 0.24035\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 197ms/step - auc: 0.5001 - loss: 0.2429 - precision: 0.2435 - recall: 0.2222 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.1547 - val_recall: 0.0985 - learning_rate: 2.5000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.5125 - loss: 0.2401 - precision: 0.2369 - recall: 0.2186\n",
      "Epoch 23: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 198ms/step - auc: 0.5046 - loss: 0.2427 - precision: 0.2347 - recall: 0.2073 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.2240 - val_recall: 0.4278 - learning_rate: 1.2500e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - auc: 0.4816 - loss: 0.2416 - precision: 0.2375 - recall: 0.2276\n",
      "Epoch 24: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 196ms/step - auc: 0.4938 - loss: 0.2430 - precision: 0.2414 - recall: 0.2243 - val_auc: 0.5000 - val_loss: 0.2404 - val_precision: 0.2587 - val_recall: 0.3294 - learning_rate: 1.2500e-04\n",
      "Epoch 24: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Fase 1 completada\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE_INITIAL),\n",
    "    loss=weighted_bce_loss,  # Cambiado de focal_loss a weighted_bce_loss\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc', multi_label=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(filepath=str(MODELS_DIR / 'model_phase1_best.h5'), monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"Modelo compilado - FASE 1: Training inicial con Weighted BCE Loss\")\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=INITIAL_EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Fase 1 completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ad84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASE 2: Fine-tuning con ultimas 40 capas descongeladas\n",
      "Epoch 1/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - auc: 0.4768 - loss: 0.2639 - precision: 0.1856 - recall: 0.2744\n",
      "Epoch 1: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 238ms/step - auc: 0.4829 - loss: 0.2529 - precision: 0.1991 - recall: 0.2833 - val_auc: 0.5074 - val_loss: 0.2404 - val_precision: 0.4213 - val_recall: 0.2683 - learning_rate: 5.0000e-05\n",
      "Epoch 2/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - auc: 0.4861 - loss: 0.2437 - precision: 0.2146 - recall: 0.2719\n",
      "Epoch 2: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 224ms/step - auc: 0.4942 - loss: 0.2447 - precision: 0.2186 - recall: 0.2674 - val_auc: 0.4966 - val_loss: 0.2404 - val_precision: 0.4213 - val_recall: 0.2683 - learning_rate: 5.0000e-05\n",
      "Epoch 3/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - auc: 0.5066 - loss: 0.2446 - precision: 0.2187 - recall: 0.2550\n",
      "Epoch 3: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 224ms/step - auc: 0.5028 - loss: 0.2440 - precision: 0.2234 - recall: 0.2540 - val_auc: 0.4925 - val_loss: 0.2406 - val_precision: 0.3167 - val_recall: 0.3548 - learning_rate: 5.0000e-05\n",
      "Epoch 4/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - auc: 0.5050 - loss: 0.2451 - precision: 0.2348 - recall: 0.2469\n",
      "Epoch 4: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 226ms/step - auc: 0.5005 - loss: 0.2441 - precision: 0.2305 - recall: 0.2455 - val_auc: 0.4939 - val_loss: 0.2406 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 5.0000e-05\n",
      "Epoch 5/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - auc: 0.4947 - loss: 0.2439 - precision: 0.2263 - recall: 0.2262\n",
      "Epoch 5: val_loss did not improve from 0.24035\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 222ms/step - auc: 0.5016 - loss: 0.2438 - precision: 0.2232 - recall: 0.2264 - val_auc: 0.5041 - val_loss: 0.2406 - val_precision: 0.2236 - val_recall: 0.2088 - learning_rate: 5.0000e-05\n",
      "Epoch 6/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - auc: 0.4749 - loss: 0.2413 - precision: 0.2473 - recall: 0.2452\n",
      "Epoch 6: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 227ms/step - auc: 0.4892 - loss: 0.2439 - precision: 0.2458 - recall: 0.2402 - val_auc: 0.4971 - val_loss: 0.2405 - val_precision: 0.1143 - val_recall: 0.0136 - learning_rate: 2.5000e-05\n",
      "Epoch 7/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - auc: 0.5056 - loss: 0.2409 - precision: 0.2521 - recall: 0.2372\n",
      "Epoch 7: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 224ms/step - auc: 0.5097 - loss: 0.2430 - precision: 0.2523 - recall: 0.2480 - val_auc: 0.4937 - val_loss: 0.2405 - val_precision: 0.2880 - val_recall: 0.3667 - learning_rate: 2.5000e-05\n",
      "Epoch 8/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - auc: 0.5035 - loss: 0.2428 - precision: 0.2510 - recall: 0.2722\n",
      "Epoch 8: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 223ms/step - auc: 0.5081 - loss: 0.2433 - precision: 0.2419 - recall: 0.2568 - val_auc: 0.5016 - val_loss: 0.2404 - val_precision: 0.3483 - val_recall: 0.1053 - learning_rate: 2.5000e-05\n",
      "Epoch 9/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - auc: 0.5011 - loss: 0.2439 - precision: 0.2257 - recall: 0.2195\n",
      "Epoch 9: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 223ms/step - auc: 0.5064 - loss: 0.2431 - precision: 0.2352 - recall: 0.2268 - val_auc: 0.5060 - val_loss: 0.2404 - val_precision: 0.3611 - val_recall: 0.0221 - learning_rate: 2.5000e-05\n",
      "Epoch 10/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - auc: 0.5000 - loss: 0.2422 - precision: 0.2494 - recall: 0.2453\n",
      "Epoch 10: val_loss did not improve from 0.24035\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 226ms/step - auc: 0.5054 - loss: 0.2431 - precision: 0.2463 - recall: 0.2427 - val_auc: 0.5006 - val_loss: 0.2405 - val_precision: 0.1243 - val_recall: 0.1579 - learning_rate: 2.5000e-05\n",
      "Epoch 11/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - auc: 0.5002 - loss: 0.2436 - precision: 0.2294 - recall: 0.2272\n",
      "Epoch 11: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 228ms/step - auc: 0.5015 - loss: 0.2432 - precision: 0.2181 - recall: 0.2165 - val_auc: 0.5010 - val_loss: 0.2405 - val_precision: 0.1416 - val_recall: 0.1070 - learning_rate: 1.2500e-05\n",
      "Epoch 12/40\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - auc: 0.5016 - loss: 0.2439 - precision: 0.2122 - recall: 0.2028\n",
      "Epoch 12: val_loss did not improve from 0.24035\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 232ms/step - auc: 0.5035 - loss: 0.2433 - precision: 0.2293 - recall: 0.2141 - val_auc: 0.4949 - val_loss: 0.2405 - val_precision: 0.1547 - val_recall: 0.0985 - learning_rate: 1.2500e-05\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Fase 2 completada\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "fine_tune_at = len(base_model.layers) - 40\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE_FINETUNING),\n",
    "    loss=weighted_bce_loss,  # Cambiado de focal_loss a weighted_bce_loss\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc', multi_label=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_datagen_ft = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest',\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "print(f\"FASE 2: Fine-tuning con ultimas {len(base_model.layers) - fine_tune_at} capas descongeladas\")\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_datagen_ft.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=FINETUNING_EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Fase 2 completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99776267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 353ms/step\n",
      "Threshold optimo (global): 0.50\n",
      "F1-micro max (global): 0.3278\n",
      "Thresholds por clase (promedio): 0.20\n",
      "Thresholds por clase (min-max): 0.20 - 0.20\n",
      "\n",
      "METRICAS FINALES EN VALIDACION\n",
      "hamming_loss: 0.9215\n",
      "subset_accuracy: 0.0000\n",
      "f1_micro: 0.1456\n",
      "f1_macro: 0.1365\n",
      "precision_micro: 0.0785\n",
      "recall_micro: 1.0000\n",
      "Tasa de positivos predichos: 1.0000\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = model.predict(X_val, verbose=1)\n",
    "\n",
    "# Buscar umbral optimo global por F1-micro (rango AMPLIADO 0.3-0.7)\n",
    "thresholds = np.arange(0.3, 0.75, 0.05)\n",
    "f1_scores = []\n",
    "for thresh in thresholds:\n",
    "    y_val_pred_binary = (y_val_pred >= thresh).astype(int)\n",
    "    f1 = f1_score(y_val, y_val_pred_binary, average='micro', zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "best_idx = int(np.argmax(f1_scores))\n",
    "best_threshold = float(thresholds[best_idx])\n",
    "print(f\"Threshold optimo (global): {best_threshold:.2f}\")\n",
    "print(f\"F1-micro max (global): {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# Umbral optimo por clase (rango ajustado)\n",
    "best_thresholds = []\n",
    "for c in range(NUM_CLASSES):\n",
    "    f1_c = []\n",
    "    for thresh in thresholds:\n",
    "        pred_c = (y_val_pred[:, c] >= thresh).astype(int)\n",
    "        f1_c.append(f1_score(y_val[:, c], pred_c, average='binary', zero_division=0))\n",
    "    best_thresholds.append(float(thresholds[int(np.argmax(f1_c))]))\n",
    "\n",
    "best_thresholds = np.array(best_thresholds)\n",
    "print(f\"Thresholds por clase (promedio): {best_thresholds.mean():.2f}\")\n",
    "print(f\"Thresholds por clase (min-max): {best_thresholds.min():.2f} - {best_thresholds.max():.2f}\")\n",
    "\n",
    "# Metricas finales con thresholds por clase\n",
    "y_val_pred_binary = (y_val_pred >= best_thresholds).astype(int)\n",
    "positive_rate = y_val_pred_binary.mean()\n",
    "metrics_phase2 = {\n",
    "    'hamming_loss': hamming_loss(y_val, y_val_pred_binary),\n",
    "    'subset_accuracy': accuracy_score(y_val, y_val_pred_binary),\n",
    "    'f1_micro': f1_score(y_val, y_val_pred_binary, average='micro', zero_division=0),\n",
    "    'f1_macro': f1_score(y_val, y_val_pred_binary, average='macro', zero_division=0),\n",
    "    'f1_samples': f1_score(y_val, y_val_pred_binary, average='samples', zero_division=0),\n",
    "    'precision_micro': precision_score(y_val, y_val_pred_binary, average='micro', zero_division=0),\n",
    "    'recall_micro': recall_score(y_val, y_val_pred_binary, average='micro', zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"\\nMETRICAS FINALES EN VALIDACION\")\n",
    "for metric, value in metrics_phase2.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "print(f\"Tasa de positivos predichos: {positive_rate:.4f}\")\n",
    "print(f\"Tasa de positivos reales: {y_val.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3199d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado\n",
      "Resultados guardados\n"
     ]
    }
   ],
   "source": [
    "model.save(MODELS_DIR / 'voc_multilabel_final.h5')\n",
    "model.save(MODELS_DIR / 'voc_multilabel_final.keras')\n",
    "print(f\"Modelo guardado\")\n",
    "\n",
    "with open(MODELS_DIR / 'training_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'metrics': metrics_phase2,\n",
    "        'config': {\n",
    "            'initial_epochs': INITIAL_EPOCHS,\n",
    "            'finetuning_epochs': FINETUNING_EPOCHS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'img_size': IMG_SIZE,\n",
    "            'learning_rate_initial': LEARNING_RATE_INITIAL,\n",
    "            'learning_rate_finetuning': LEARNING_RATE_FINETUNING\n",
    "        },\n",
    "        'thresholds': best_thresholds.tolist()\n",
    "    }, f, indent=2)\n",
    "print(f\"Resultados guardados\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
